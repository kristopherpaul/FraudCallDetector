{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "299c3e77195148539d6f60384f68a439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_21bd34094e724e179bcd6eb7c6357677",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7e9c0640192145d2817dbf62f24fd919",
              "IPY_MODEL_5f712d3c987a493fb14fdc31adddbad8"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "21bd34094e724e179bcd6eb7c6357677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "7e9c0640192145d2817dbf62f24fd919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_46a696e072c748bd8cb3698e7a4b9d92",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fae85cf2e3744d1a1283b4b9801af7d"
          },
          "model_module_version": "1.5.0"
        },
        "5f712d3c987a493fb14fdc31adddbad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4709c2247c7d4aff8e3dca212a6a5da5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:00&lt;00:00, 1.60MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8659fab04924958b7411b5e9982b072"
          },
          "model_module_version": "1.5.0"
        },
        "46a696e072c748bd8cb3698e7a4b9d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "5fae85cf2e3744d1a1283b4b9801af7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "4709c2247c7d4aff8e3dca212a6a5da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "c8659fab04924958b7411b5e9982b072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "5c02c7736efa4fe481c4dc6856255337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e6837dc4da9d42449d1511579eed1beb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d02637b99a594b34864db1acefe77b39",
              "IPY_MODEL_7dd1c0be733d4ed88e3e3b301f70a9d1"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "e6837dc4da9d42449d1511579eed1beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "d02637b99a594b34864db1acefe77b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2df4c0b27754e81aceebc9e8a875186",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ed1f0b298634ac3956c1f3953064fbf"
          },
          "model_module_version": "1.5.0"
        },
        "7dd1c0be733d4ed88e3e3b301f70a9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d54e99ed54554ba2974ee0cac51ecbbc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760/760 [00:00&lt;00:00, 1.39kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab71590d4c6b4f1fb24382e563a460e6"
          },
          "model_module_version": "1.5.0"
        },
        "d2df4c0b27754e81aceebc9e8a875186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "6ed1f0b298634ac3956c1f3953064fbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "d54e99ed54554ba2974ee0cac51ecbbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "ab71590d4c6b4f1fb24382e563a460e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "2306d1d0455e473c89b92f05e205aa36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8637435fd39b4a23871b7cdcbbb420e3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cb9e534ed3f84ae6b34b72cf3bdbcb3a",
              "IPY_MODEL_422c2b60a1024497985b28ddef33192b"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "8637435fd39b4a23871b7cdcbbb420e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "cb9e534ed3f84ae6b34b72cf3bdbcb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e020c967536439d99ab254a68f836ba",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 467042463,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 467042463,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45bdb7f7b1dd40808d14cd0999f0de75"
          },
          "model_module_version": "1.5.0"
        },
        "422c2b60a1024497985b28ddef33192b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_740e8236257b455c86cafd23f6338e1e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 467M/467M [09:17&lt;00:00, 838kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23c5f57658e347758d3aaa5cb4e434d7"
          },
          "model_module_version": "1.5.0"
        },
        "2e020c967536439d99ab254a68f836ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "45bdb7f7b1dd40808d14cd0999f0de75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "740e8236257b455c86cafd23f6338e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "23c5f57658e347758d3aaa5cb4e434d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristopherpaul/FraudCallDetector/blob/main/AIFraudCall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAAFYPVYysxs"
      },
      "source": [
        "**Steps**\n",
        "1. Scraping relevant tweets from twitter\n",
        "> This is stored in **Data/raw_scraped_tweets.txt** and was done by **scrape_tweets.py** program\n",
        "2. Normalizing Text of scraped tweets and the Santa Barbara Corpus of regular conversations \n",
        "3. Training the XLNet Transformer Model.\n",
        "4. Converting Call recordings of the final test set to transcripts with Speaker Diarization using IBM Watson's API\n",
        "5. Normalizing Text of the final test set transcripts\n",
        "6. Getting Model predictions on the Test set of 20 Call Recordings(10 fraud + 10 regular)\n",
        "7. Evaluating Model's effectiveness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0GbMGLGU0O9"
      },
      "source": [
        "**Setting up the environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MJeMRvXT_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc3a96c-9725-4034-82ad-3fb536c69f76"
      },
      "source": [
        "!pip install symspellpy\n",
        "!pip install pylangacq\n",
        "!pip install ibm_watson\n",
        "!pip install phonetics\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "%cd drive\n",
        "%cd My Drive\n",
        "%cd DLFraudCall"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting symspellpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/af/e71fcca6a42b6a63f518b0c1627e1f67822815cb0cf71e6af05acbd75c78/symspellpy-6.7.0-py3-none-any.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.18.5)\n",
            "Installing collected packages: symspellpy\n",
            "Successfully installed symspellpy-6.7.0\n",
            "Collecting pylangacq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/80/a86b86562e0c233babf9d63c6189917eac6f5c4ebe5119b52b2448208073/pylangacq-0.12.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pylangacq\n",
            "Successfully installed pylangacq-0.12.0\n",
            "Collecting ibm_watson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/3c/c2cfb41db546fe98820e89017c892d73991cef61b9c48680191fe703a214/ibm-watson-4.7.1.tar.gz (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from ibm_watson) (2.23.0)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from ibm_watson) (2.8.1)\n",
            "Collecting websocket-client==0.48.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 14.6MB/s \n",
            "\u001b[?25hCollecting ibm_cloud_sdk_core==1.7.3\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/23/aa9ae242f6348a1ed28fca2e6d3e76e043c3db951f9b516e1992518fe2c3/ibm-cloud-sdk-core-1.7.3.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm_watson) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm_watson) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2020.6.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python_dateutil>=2.5.3->ibm_watson) (1.15.0)\n",
            "Collecting PyJWT>=1.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: ibm-watson, ibm-cloud-sdk-core\n",
            "  Building wheel for ibm-watson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-watson: filename=ibm_watson-4.7.1-cp36-none-any.whl size=379133 sha256=0e91d05e99f381ddd8aa8293812248bfe9c59c136d54a87fb5d85d22e82fc4c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/14/69/dbbd573a3bab3bf64984572284f13f174f430038308abdd73c\n",
            "  Building wheel for ibm-cloud-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-1.7.3-cp36-none-any.whl size=45945 sha256=bfc5bb53b413914fa337b14daecfa639780f7ffc1f2db556766210affc941984\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/6e/58/589e0f841c2fae9dad99630d78ddc7a60c5c7663a16a39cdbb\n",
            "Successfully built ibm-watson ibm-cloud-sdk-core\n",
            "Installing collected packages: websocket-client, PyJWT, ibm-cloud-sdk-core, ibm-watson\n",
            "Successfully installed PyJWT-1.7.1 ibm-cloud-sdk-core-1.7.3 ibm-watson-4.7.1 websocket-client-0.48.0\n",
            "Collecting phonetics\n",
            "  Downloading https://files.pythonhosted.org/packages/67/a5/d1b6dbcbb05477aa5f0c5e73a7d68c6d23ab098af4461072f00999ed573a/phonetics-1.0.5.tar.gz\n",
            "Building wheels for collected packages: phonetics\n",
            "  Building wheel for phonetics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phonetics: filename=phonetics-1.0.5-py2.py3-none-any.whl size=8713 sha256=65b4dd8376097fb77bb5d3ed28fc4911204361e85b3b70893a72f6b1c32c2539\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/ca/b1/e41e662f86b95d185748da378fdc995c1520b0d5628c46a0b7\n",
            "Successfully built phonetics\n",
            "Installing collected packages: phonetics\n",
            "Successfully installed phonetics-1.0.5\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 8.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=35817613197e8fe7303e4cbf1fd4283216833426140f325a076f93de45c67deb\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "/content/drive\n",
            "/content/drive/My Drive\n",
            "/content/drive/My Drive/DLFraudCall\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDu7w_N26nRB"
      },
      "source": [
        "**Required Library Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuzghYTfVkl2"
      },
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "from tqdm import trange\n",
        "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import XLNetForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "from ibm_watson import SpeechToTextV1\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "from numpy import array,asarray,zeros\n",
        "import pylangacq as pla\n",
        "import spacy\n",
        "import pkg_resources\n",
        "import pickle\n",
        "import random \n",
        "import string\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "dirpath = \"drive/My Drive/DLFraudCall\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF121nRi62q_"
      },
      "source": [
        "**Converting Santa Barabara corpus(SBC) collection of regular call transcripts from .cha to .txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-9Dw4x4G7a"
      },
      "source": [
        "cha_f = pla.read_chat('./Data/SantaBarbaraCorpus/*.cha')\n",
        "cha_sents = cha_f.utterances()\n",
        "\n",
        "cha_d = {}\n",
        "\n",
        "for name,sent in cha_sents:\n",
        "\t\tif name in cha_d.keys():\n",
        "\t\t\t\tcha_d[name] += \" \"+sent\n",
        "\t\telse:\n",
        "\t\t\t\tcha_d[name] = sent\n",
        "\n",
        "with open('./Data/raw_SBC.txt', 'w',encoding=\"utf-8\") as filehandle:\n",
        "    filehandle.writelines(\"%s\\n\" % value for key,value in cha_d.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcNWnKNU7xr4"
      },
      "source": [
        "**Functions for Text Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cup8MFy5DAEV"
      },
      "source": [
        "1. Normalizing Punctuation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-sh4TrZ9o6V"
      },
      "source": [
        "def norm_punctuation(data,b):\n",
        "    norm_data = []\n",
        "    whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "    for line in data:\n",
        "        line = str(line)\n",
        "        line = re.sub('\\(','',line)\n",
        "        line = re.sub('\\)','',line)\n",
        "        line = re.sub('’','\\'',line)\n",
        "        line = re.sub(',',' ',line)\n",
        "        line = re.sub('‘','\\'',line)\n",
        "        line = re.sub('\\.',' ',line)\n",
        "        line = re.sub('%HESITATION','',line)\n",
        "        line = re.sub('\\'*\\'','',line)\n",
        "        line = re.sub(r'([!?,;])\\1+', r'\\1', line)\n",
        "        line = re.sub(r'\\.{2,}', r'...', line)\n",
        "        if b:\n",
        "            #Only for SBC Data\n",
        "            line = ''.join(filter(whitelist.__contains__,line))\n",
        "        norm_data.append(line)\n",
        "    return norm_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ_7F9onDUZx"
      },
      "source": [
        "2. Removing Tags like **@userid** mainly from tweets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUNnDIqNDf8C"
      },
      "source": [
        "def rem_tag(data):\n",
        "    norm_data = []\n",
        "    for line in data:\n",
        "        line = str(line)\n",
        "        line = re.sub(r'@[A-Za-z0-9\\.\\-+_]+', r'', line)\n",
        "        norm_data.append(line)\n",
        "    return norm_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Vi9WubDw5B"
      },
      "source": [
        "3. Normalizing Whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTJds76cD6uO"
      },
      "source": [
        "def norm_whitespace(data):\n",
        "    norm_data = []\n",
        "    for line in data:\n",
        "        line = str(line)\n",
        "        line = re.sub(r\"//t\",r\"\\t\", line)\n",
        "        line = re.sub(r\"( )\\1+\",r\"\\1\", line)\n",
        "        line = re.sub(r\"(\\n)\\1+\",r\"\\1\", line)\n",
        "        line = re.sub(r\"(\\r)\\1+\",r\"\\1\", line)\n",
        "        line = re.sub(r\"(\\t)\\1+\",r\"\\1\", line)\n",
        "        norm_data.append(line.strip(\" \"))\n",
        "    return norm_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dZ6Rdt-D_Aj"
      },
      "source": [
        "4. Normalizing Character cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc6Xn5mCEDyJ"
      },
      "source": [
        "def norm_case(data):\n",
        "    norm_data = []\n",
        "    for line in data:\n",
        "        line = str(line)\n",
        "        line = line.lower()\n",
        "        norm_data.append(line)\n",
        "    return norm_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8_9FobsEG3y"
      },
      "source": [
        "5. Expanding Contractions eg: **we're** is replaced with **we are**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDt8HwWhEbSt"
      },
      "source": [
        "def other_contrac(data):\n",
        "    othercon = json.loads(open('./NLP_txt/othercon.json', 'r').read())\n",
        "    norm_data = []\n",
        "    for line in data:\n",
        "        tokens = line.split()\n",
        "        new_tokens = []\n",
        "        for t_pos in range(0,len(tokens)):\n",
        "            if tokens[t_pos] in othercon:\n",
        "                new_tokens.append(othercon[tokens[t_pos]])\n",
        "            else:\n",
        "                new_tokens.append(tokens[t_pos])\n",
        "        new_line = \" \".join(new_tokens).strip(\" \")\n",
        "        norm_data.append(new_line)\n",
        "    return norm_data\n",
        "\n",
        "def norm_contractions(data):\n",
        "    stdcon = json.loads(open('./NLP_txt/stdcon.json', 'r').read())\n",
        "    norm_data = []\n",
        "    for line in data:\n",
        "        tokens = line.split()\n",
        "        new_tokens = []\n",
        "        skip = False\n",
        "        for t_pos in range(0,len(tokens)):\n",
        "            if skip:\n",
        "                skip = False\n",
        "                continue\n",
        "            if tokens[t_pos] in stdcon:\n",
        "                new_tokens.append(stdcon[tokens[t_pos]])\n",
        "            elif (t_pos < (len(tokens)-1)) and (str(tokens[t_pos]+\"'\"+tokens[t_pos+1]) in stdcon):\n",
        "                new_tokens.append(stdcon[str(tokens[t_pos]+\"'\"+tokens[t_pos+1])])\n",
        "                skip = True\n",
        "            else:\n",
        "                new_tokens.append(tokens[t_pos])\n",
        "        new_line = \" \".join(new_tokens).strip(\" \")\n",
        "        norm_data.append(new_line)\n",
        "    return norm_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SrfMQeHEek6"
      },
      "source": [
        "6. Spelling Corrections along with reducing exaggerations eg: **ohhh** is replaced with **oh**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEOGyQ18Endm"
      },
      "source": [
        "def spell_correction(data):\n",
        "    mx_edit_dist = 3\n",
        "    pref_len = 4\n",
        "    spellchecker = SymSpell(mx_edit_dist,pref_len)\n",
        "    dictionary_path = pkg_resources.resource_filename(\"symspellpy\",\"frequency_dictionary_en_82_765.txt\")\n",
        "    bigram_path = pkg_resources.resource_filename(\"symspellpy\",\"frequency_bigramdictionary_en_243_342.txt\")\n",
        "    spellchecker.load_dictionary(dictionary_path,term_index=0,count_index=1)\n",
        "    spellchecker.load_bigram_dictionary(dictionary_path,term_index=0,count_index=2)\n",
        "    norm_data = []\n",
        "    for line in data:\n",
        "        norm_data.append(spell_correction_line(line,spellchecker))\n",
        "    return norm_data\n",
        "\n",
        "def reduce_exaggeration(line):\n",
        "    line = str(line)\n",
        "    return re.sub(r'([\\w])\\1+', r'\\1', line)\n",
        "\n",
        "def is_numeric(line):\n",
        "    for char in line:\n",
        "        if not (char in \"0123456789\" or char in \",%.$\"):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def spell_correction_line(line,spellchecker):\n",
        "    if len(line) < 1:\n",
        "        return \"\"\n",
        "    mx_edit_dist_l = 2\n",
        "    suggest_verbosity = Verbosity.TOP\n",
        "    token_list = line.split()\n",
        "    for word_pos in range(len(token_list)):\n",
        "        word = token_list[word_pos]\n",
        "        if word is None:\n",
        "            token_list[word_pos] = \"\"\n",
        "            continue\n",
        "        if not '\\n' in word and word not in string.punctuation and not is_numeric(word) and not (word in spellchecker.words.keys()):\n",
        "            suggestions = spellchecker.lookup(word,suggest_verbosity,mx_edit_dist_l)\n",
        "            n_word = \"\"\n",
        "            if len(suggestions) > 0:\n",
        "                n_word = suggestions[0].term\n",
        "            else:\n",
        "                n_word = reduce_exaggeration(word)\n",
        "            token_list[word_pos] = n_word\n",
        "    return \" \".join(token_list).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgttZPNAFgj1"
      },
      "source": [
        "7. Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yateWso7Fsm4"
      },
      "source": [
        "def rem_pre_stopwords(data):\n",
        "    new_data = []\n",
        "    stopwords = []\n",
        "    with open('./NLP_txt/pre_stopwords.txt', 'r') as filehandle:\n",
        "        stopwords = [word.strip() for word in filehandle.readlines()]\n",
        "    for line in data:\n",
        "        words = line.split(\" \")\n",
        "        new_words = []\n",
        "        for word in words:\n",
        "            if word not in stopwords:\n",
        "                new_words.append(word)\n",
        "        new_line = \" \".join(new_words).strip()\n",
        "        new_data.append(new_line)\n",
        "    return new_data\n",
        "\n",
        "def rem_stopwords(data):\n",
        "    new_data = []\n",
        "    stopwords = []\n",
        "    with open('./NLP_txt/stopwords.txt', 'r') as filehandle:\n",
        "        stopwords = [word.strip() for word in filehandle.readlines()]\n",
        "    for line in data:\n",
        "        words = line.split(\" \")\n",
        "        new_words = []\n",
        "        for word in words:\n",
        "            if word not in stopwords:\n",
        "                new_words.append(word)\n",
        "        new_line = \" \".join(new_words).strip()\n",
        "        new_data.append(new_line)\n",
        "    return new_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM83rU0zF-D9"
      },
      "source": [
        "8. Lemmatizing to group together variant forms of the same word eg: **changing** is replaced with **change**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMy1EF0FG3d9"
      },
      "source": [
        "def lemmatize(data):\n",
        "    nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
        "    new_norm=[]\n",
        "    for sentence in data:\n",
        "        new_norm.append(_lemmatize_text(sentence, nlp).strip())\n",
        "    return new_norm\n",
        "\n",
        "def _lemmatize_text(sentence, nlp):\n",
        "    sent = \"\"\n",
        "    doc = nlp(sentence)\n",
        "    for token in doc:\n",
        "        sent+=\" \"+token.lemma_\n",
        "    return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwt3JkrsHHc0"
      },
      "source": [
        "**Grouping the whole Text Normalization process into a single function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqfxdNjUHZhl"
      },
      "source": [
        "def normalize_data(data,b):\n",
        "    data = norm_punctuation(data,b)\n",
        "    data = rem_tag(data)\n",
        "    data = norm_whitespace(data)\n",
        "    data = norm_case(data)\n",
        "    data = other_contrac(data)\n",
        "    data = norm_contractions(data)\n",
        "    data = norm_case(data)\n",
        "    data = norm_whitespace(data)\n",
        "    if b:\n",
        "        data = spell_correction(data)\n",
        "    data = lemmatize(data)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data[i] = re.sub('-PRON-','',data[i])\n",
        "\n",
        "    data = norm_whitespace(data)\n",
        "    data = rem_pre_stopwords(data)\n",
        "    data = rem_stopwords(data)\n",
        "    data = set(data)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0LnJUYaNOy1"
      },
      "source": [
        "Normalizing **raw_scraped_tweets.txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6t_yPauJFv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e1718f-ebd6-4eee-a633-a76ba6c764e5"
      },
      "source": [
        "data = []\n",
        "\n",
        "with open('./Data/raw_scraped_tweets.txt', 'r',encoding=\"utf-8\") as filehandle:\n",
        "    data = [line.strip() for line in filehandle.readlines()]\n",
        "\n",
        "b_words1 = []\n",
        "for line in data:\n",
        "    for word in line.split():\n",
        "        b_words1.append(word)\n",
        "b_words1 = set(b_words1)\n",
        "\n",
        "data = normalize_data(data,False)\n",
        "\n",
        "a_words1 = []\n",
        "for line in data:\n",
        "    for word in line.split():\n",
        "        a_words1.append(word)\n",
        "a_words1 = set(a_words1)\n",
        "\n",
        "with open('./Data/norm_scraped_tweets.txt', 'w',encoding=\"utf-8\") as filehandle:\n",
        "    filehandle.writelines(\"%s\\n\" % line for line in data)\n",
        "\n",
        "print(\"raw_scraped_tweets.txt\\n---------------------\")\n",
        "print(\"No. of distinct words before Text normalization:\",len(b_words1))\n",
        "print(\"No. of distinct words after Text normalization:\",len(a_words1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw_scraped_tweets.txt\n",
            "---------------------\n",
            "No. of distinct words before Text normalization: 2337\n",
            "No. of distinct words after Text normalization: 1411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKp3THbkQBQe"
      },
      "source": [
        "Normalizing **raw_SBC.txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl4l2WpUKsRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4519d8a1-7d43-46e7-e4f2-552b9236ded7"
      },
      "source": [
        "data = []\n",
        "\n",
        "with open('./Data/raw_SBC.txt', 'r',encoding=\"utf-8\") as filehandle:\n",
        "    data = [line.strip() for line in filehandle.readlines()]\n",
        "\n",
        "b_words2 = []\n",
        "for line in data:\n",
        "    for word in line.split():\n",
        "        b_words2.append(word)\n",
        "b_words2 = set(b_words2)\n",
        "\n",
        "data = normalize_data(data,True)\n",
        "\n",
        "a_words2 = []\n",
        "for line in data:\n",
        "    for word in line.split():\n",
        "        a_words2.append(word)\n",
        "a_words2 = set(a_words2)\n",
        "\n",
        "with open('./Data/norm_SBC.txt', 'w',encoding=\"utf-8\") as filehandle:\n",
        "    filehandle.writelines(\"%s\\n\" % line for line in data)\n",
        "\n",
        "print(\"raw_SBC.txt\\n-----------\")\n",
        "print(\"No. of distinct words before Text normalization:\",len(b_words2))\n",
        "print(\"No. of distinct words after Text normalization:\",len(a_words2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw_SBC.txt\n",
            "-----------\n",
            "No. of distinct words before Text normalization: 7220\n",
            "No. of distinct words after Text normalization: 3417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piVRkXYnRQ95"
      },
      "source": [
        "After Text Normalization, there is a **40%**, **53%** reduction in the number of distinct words in **raw_scraped_tweets.txt**, **raw_SBC.txt** respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE9DibIDmd5v"
      },
      "source": [
        "Combining **raw_scraped_tweets.txt** and **raw_SBC.txt** into **Dataset.csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_CO_NgjmZid"
      },
      "source": [
        "MAX_LEN = 20\n",
        "\n",
        "data_labels = []\n",
        "\n",
        "with open('./Data/norm_scraped_tweets.txt', 'r',encoding=\"utf-8\") as filehandle:\n",
        "    data_tweets = [line.strip() for line in filehandle.readlines()]\n",
        "    data_labels.extend([1]*len(data_tweets))\n",
        "\n",
        "with open('./Data/norm_SBC.txt', 'r',encoding=\"utf-8\") as filehandle:\n",
        "    data_sbc = [line.strip() for line in filehandle.readlines()]\n",
        "    data_labels.extend([0]*len(data_sbc))\n",
        "\n",
        "dict = {'text': data_tweets+data_sbc, 'labels': data_labels}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('./Data/Dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpyB_ZNRkV7G"
      },
      "source": [
        "**XLNet** \n",
        "1. Tokenizing Sentences of the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9bRoVKXkWt0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "299c3e77195148539d6f60384f68a439",
            "21bd34094e724e179bcd6eb7c6357677",
            "7e9c0640192145d2817dbf62f24fd919",
            "5f712d3c987a493fb14fdc31adddbad8",
            "46a696e072c748bd8cb3698e7a4b9d92",
            "5fae85cf2e3744d1a1283b4b9801af7d",
            "4709c2247c7d4aff8e3dca212a6a5da5",
            "c8659fab04924958b7411b5e9982b072"
          ]
        },
        "outputId": "9c4d97f0-6111-4cfd-e6d5-1e015ff610d5"
      },
      "source": [
        "df = pd.read_csv('./Data/Dataset.csv')\n",
        "sents = df.text.values\n",
        "sents = [str(sent) + \" [SEP] [CLS]\" for sent in sents]\n",
        "labels = df.labels.values\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=True)\n",
        "tokenized_sents = [tokenizer.tokenize(sent) for sent in sents]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "299c3e77195148539d6f60384f68a439",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5ARLf1XILtl"
      },
      "source": [
        "2. Padding/truncating sequences and creating attention masks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10X4JMzwy6g7"
      },
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_sents]\n",
        "input_ids = pad_sequences(input_ids,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUSoOZ_5IU2-"
      },
      "source": [
        "3. Splitting Dataset into training and cross validation sets in the ratio 80:20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as_wogSOqrST"
      },
      "source": [
        "train_sents,cv_sents,train_labels,cv_labels = train_test_split(input_ids,labels,random_state=56,test_size=0.2)\n",
        "train_masks,cv_masks, _, _ = train_test_split(attention_masks,input_ids,random_state=56,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkZTEhOs0R1X"
      },
      "source": [
        "train_sents = torch.tensor(train_sents)\n",
        "cv_sents = torch.tensor(cv_sents)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "cv_labels = torch.tensor(cv_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "cv_masks = torch.tensor(cv_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDKCTv6CInV8"
      },
      "source": [
        "4. Loading the training and cross validation data into DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h8PuoDebJ_G"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_sents,train_masks,train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
        "\n",
        "cv_data = TensorDataset(cv_sents,cv_masks,cv_labels)\n",
        "cv_sampler = SequentialSampler(cv_data)\n",
        "cv_dataloader = DataLoader(cv_data,sampler=cv_sampler,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QMLr7SLIw7Q"
      },
      "source": [
        "5. Choosing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we4eiXsmbpZl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "5c02c7736efa4fe481c4dc6856255337",
            "e6837dc4da9d42449d1511579eed1beb",
            "d02637b99a594b34864db1acefe77b39",
            "7dd1c0be733d4ed88e3e3b301f70a9d1",
            "d2df4c0b27754e81aceebc9e8a875186",
            "6ed1f0b298634ac3956c1f3953064fbf",
            "d54e99ed54554ba2974ee0cac51ecbbc",
            "ab71590d4c6b4f1fb24382e563a460e6",
            "2306d1d0455e473c89b92f05e205aa36",
            "8637435fd39b4a23871b7cdcbbb420e3",
            "cb9e534ed3f84ae6b34b72cf3bdbcb3a",
            "422c2b60a1024497985b28ddef33192b",
            "2e020c967536439d99ab254a68f836ba",
            "45bdb7f7b1dd40808d14cd0999f0de75",
            "740e8236257b455c86cafd23f6338e1e",
            "23c5f57658e347758d3aaa5cb4e434d7"
          ]
        },
        "outputId": "d125dbf4-10b6-4d7d-f121-0d5c89347069"
      },
      "source": [
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\",num_labels=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c02c7736efa4fe481c4dc6856255337",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2306d1d0455e473c89b92f05e205aa36",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAd2uii3IzhN"
      },
      "source": [
        "6. Parameter optimizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVbxoaBOb7Os"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias','gamma','beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Ct-u4MI7zO"
      },
      "source": [
        "7. Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFSAZze5cgcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e38d19-9dfd-4e15-c2d0-ed5c57b07244"
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "train_loss_set = []\n",
        "\n",
        "epochs = 12\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        train_loss_set.append(loss.item())    \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            prediction = torch.argmax(outputs[0],dim=1)\n",
        "            total += b_labels.size(0)\n",
        "            correct+=(prediction==b_labels).sum().item()\n",
        "        print('Train Accuracy of the model on train data is: {} %'.format(100 * correct / total))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, batch in enumerate(cv_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            prediction = torch.argmax(outputs[0],dim=1)\n",
        "            total += b_labels.size(0)\n",
        "            correct+=(prediction==b_labels).sum().item()\n",
        "        print('Cross Validation Accuracy of the model on cv data is: {} %'.format(100 * correct / total))\n",
        "        cv_acc = (100.0 * correct) / total\n",
        "        if cv_acc >= 93:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/12 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy of the model on train data is: 87.62214983713355 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   8%|▊         | 1/12 [01:29<16:20, 89.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy of the model on cv data is: 83.11688311688312 %\n",
            "Train Accuracy of the model on train data is: 87.62214983713355 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  17%|█▋        | 2/12 [02:57<14:47, 88.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy of the model on cv data is: 83.11688311688312 %\n",
            "Train Accuracy of the model on train data is: 96.09120521172639 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 3/12 [04:25<13:18, 88.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy of the model on cv data is: 89.6103896103896 %\n",
            "Train Accuracy of the model on train data is: 97.06840390879479 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 4/12 [05:54<11:49, 88.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy of the model on cv data is: 89.6103896103896 %\n",
            "Train Accuracy of the model on train data is: 99.0228013029316 %\n",
            "Cross Validation Accuracy of the model on cv data is: 93.50649350649351 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uef4q5eMXRHF"
      },
      "source": [
        "**Speech to Text conversion of Call Recordings with Speaker Diarization using IBM Watson's API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zDu6evIak22"
      },
      "source": [
        "1. Sending API request and receving json response"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLbdEvaCXd3t"
      },
      "source": [
        "apikey = \"Ybjtzt1yFtMNCNvU1dEOVUlSmpIJIcjRwly5WU5YZNbF\"\n",
        "url = \"https://api.au-syd.speech-to-text.watson.cloud.ibm.com/instances/70141360-b930-4968-bd81-ccf647e25b31\"\n",
        "\n",
        "authenticator = IAMAuthenticator(apikey)\n",
        "speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
        "speech_to_text.set_service_url(url)\n",
        "\n",
        "file_done = []\n",
        "for filename in os.listdir('./SpeechToText/json'):\n",
        "\t  file_done.append(filename.split('.')[0])\n",
        "\n",
        "for filename in os.listdir('./CallRecordings'):\n",
        "    if filename.split('.')[0] in file_done:\n",
        "        continue\n",
        "    print(\"Converting\",filename,\"....\",end=\" \")\n",
        "    f_type = filename.split('.')[1]\n",
        "    file_path = \"./CallRecordings/\"+filename\n",
        "    with open(file_path, 'rb') as f:\n",
        "\t      c_type = 'audio/'+f_type\n",
        "\t      res = speech_to_text.recognize(audio=f,content_type=c_type,model='en-US_NarrowbandModel',continuous=True,speaker_labels=True).get_result()\n",
        "\t      out_json = json.dumps(res,indent=2)\n",
        "\n",
        "    out_file = filename.split('.')[0]+\".json\"\n",
        "    out_path = \"./SpeechToText/json/\"+out_file\n",
        "    with open(out_path, \"w\") as outfile: \n",
        "\t      outfile.write(out_json)\n",
        "    print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cfJrGWCay3W"
      },
      "source": [
        "2. Extracting text from the json response and normalizing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3uO0p5ta6DF"
      },
      "source": [
        "file_done = []\n",
        "for filename in os.listdir(\"./SpeechToText/transcript\"):\n",
        "\t  file_done.append(filename.split('.')[0])\n",
        "\n",
        "for filename in os.listdir(\"./SpeechToText/json\"):\n",
        "    if filename.split('.')[0] in file_done:\n",
        "        continue\n",
        "    print(\"Normalizing\",filename,\"....\",end=\" \")\t\n",
        "    file_path = \"./SpeechToText/json/\"+filename\n",
        "    with open(file_path, 'r') as f:\n",
        "        json_obj = json.load(f)\n",
        "        \n",
        "    ttow = {}\n",
        "    for sentence in json_obj['results']:\n",
        "\t\t    text = sentence['alternatives'][0]['transcript']\n",
        "\t\t    for word in sentence['alternatives'][0]['timestamps']:\n",
        "\t\t\t      ttow[word[1]] = {}\n",
        "\t\t\t      ttow[word[1]][word[2]] = word[0]\n",
        "\t\n",
        "    speakers = {}\n",
        "    for word in json_obj['speaker_labels']:\n",
        "\t\t    confi = word['confidence']\n",
        "\t\t    speaker = word['speaker']\n",
        "\t\t    if speaker in speakers:\n",
        "\t\t\t      speakers[speaker] += \" \"+ttow[word['from']][word['to']]\n",
        "\t\t    else:\n",
        "\t\t\t      speakers[speaker] = ttow[word['from']][word['to']]\n",
        "\n",
        "    out_file = filename.split('.')[0]+\".txt\"\n",
        "    out_path = \"./SpeechToText/transcript/\"+out_file\n",
        "    with open(out_path, \"w\") as outfile:\n",
        "        data = [] \n",
        "        for speaker in speakers.keys():\n",
        "            data.append(speakers[speaker])\n",
        "        data = normalize_data(data,True)\n",
        "        for line in data:\n",
        "            outfile.write(line+\"\\n\")\n",
        "    print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsbwZip4pJOI"
      },
      "source": [
        "**Getting Model Predictions for the Call recordings in the final Test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbE1ciHopK5E"
      },
      "source": [
        "for filename in os.listdir(\"./SpeechToText/transcript\"):\n",
        "    data = []\n",
        "    data_labels = []\n",
        "    with open('./SpeechToText/transcript/'+filename, 'r') as filehandle:\n",
        "        for line in filehandle.readlines():\n",
        "            if len(line.strip().split()) >= 4:\n",
        "                data.append(line.strip())\n",
        "\n",
        "    data = [' '.join(data)]\n",
        "    data_labels.append(1)\n",
        "    sents = data\n",
        "    sents = [str(sent) + \" [SEP] [CLS]\" for sent in sents]\n",
        "    labels = data_labels\n",
        "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased',do_lower_case=True)\n",
        "    tokenized_sents = [tokenizer.tokenize(sent) for sent in sents]\n",
        "\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_sents]\n",
        "    input_ids = pad_sequences(input_ids,maxlen=MAX_LEN,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)    \n",
        "\n",
        "    test_sents = input_ids\n",
        "    test_labels = labels\n",
        "    test_masks = attention_masks\n",
        "\n",
        "    test_sents = torch.tensor(test_sents)\n",
        "    test_labels = torch.tensor(test_labels)\n",
        "    test_masks = torch.tensor(test_masks)\n",
        "\n",
        "\n",
        "    test_data = TensorDataset(test_sents,test_masks,test_labels)\n",
        "    test_sampler = SequentialSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, batch in enumerate(test_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            prediction = torch.argmax(outputs[0],dim=1)\n",
        "            total += b_labels.size(0)\n",
        "            correct+=(prediction==b_labels).sum().item()\n",
        "    probability = 100 * correct / total   \n",
        "    out_file = filename.split('.')[0]+\".pred\"\n",
        "    out_path = \"./Predictions/\"+out_file\n",
        "    with open(out_path, \"w\") as outfile:\n",
        "        outfile.write(str(probability))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktjBBWMaJa1H"
      },
      "source": [
        "**Evaluating Model's effectiveness**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo5cwWhEoJjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7136b45-9c4d-47b4-9697-3f3bd41e8e6e"
      },
      "source": [
        "print(\"File Name                      |Type of Recording|Predicted Type\")\n",
        "print(\"------------------------------- ----------------- --------------\")\n",
        "correct = 0\n",
        "total = 0\n",
        "for filename in os.listdir(\"./Predictions\"):\n",
        "    typerec = \"NOT FRAUD\"\n",
        "    if '-' in filename:\n",
        "        typerec = \"FRAUD\"\n",
        "    typepred = \"NOT FRAUD\"\n",
        "    with open('./Predictions/'+filename, 'r') as filehandle:\n",
        "        for line in filehandle.readlines():\n",
        "            val = float(line.strip())\n",
        "            if(val == 100.0):\n",
        "                typepred = \"FRAUD\"\n",
        "    print(filename+(\" \"*(31-len(filename)))+\"|\"+typerec+(\" \"*(17-len(typerec)))+\"|\"+typepred+(\" \"*(14-len(typepred))))\n",
        "    total += 1\n",
        "    correct += (typerec==typepred)\n",
        "print(\"Accuracy: \",(correct/total)*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                      |Type of Recording|Predicted Type\n",
            "------------------------------- ----------------- --------------\n",
            "elder-fraud.pred               |FRAUD            |NOT FRAUD     \n",
            "amazon-fraud.pred              |FRAUD            |FRAUD         \n",
            "cv19_delivery-fraud.pred       |FRAUD            |FRAUD         \n",
            "cv19_social_security-fraud.pred|FRAUD            |FRAUD         \n",
            "debt_arrest-fraud.pred         |FRAUD            |NOT FRAUD     \n",
            "diabetic_test_kit-fraud.pred   |FRAUD            |FRAUD         \n",
            "student_loan-fraud.pred        |FRAUD            |FRAUD         \n",
            "irs_tax-fraud.pred             |FRAUD            |FRAUD         \n",
            "test_kit-fraud.pred            |FRAUD            |FRAUD         \n",
            "cv19_vaccine-fraud.pred        |FRAUD            |FRAUD         \n",
            "SBC025.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC028.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC031.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC039.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC042.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC041.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC044.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC047.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC052.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "SBC058.pred                    |NOT FRAUD        |NOT FRAUD     \n",
            "Accuracy:  90.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}